{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6eae64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bharathvelamala/Documents/projects/model_inference/ApacheBeamInferenceML/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "from typing import Iterable\n",
    "from typing import Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForMaskedLM\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.ml.inference.base import KeyedModelHandler\n",
    "from apache_beam.ml.inference.base import PredictionResult\n",
    "from apache_beam.ml.inference.base import RunInference\n",
    "from apache_beam.ml.inference.huggingface_inference import HuggingFacePipelineModelHandler\n",
    "from apache_beam.ml.inference.huggingface_inference import HuggingFaceModelHandlerKeyedTensor\n",
    "from apache_beam.ml.inference.huggingface_inference import HuggingFaceModelHandlerTensor\n",
    "from apache_beam.ml.inference.huggingface_inference import PipelineTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39e905e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HuggingFaceModelHandler specified a 'GPU' device, but GPUs are not available. Switching to CPU.\n",
      "HuggingFaceModelHandler specified a 'GPU' device, but GPUs are not available. Switching to CPU.\n"
     ]
    }
   ],
   "source": [
    "model_handler = HuggingFacePipelineModelHandler(\n",
    "    task=PipelineTask.Translation_XX_to_YY,\n",
    "    model = \"google/flan-t5-small\",\n",
    "    load_pipeline_args={'framework': 'pt'},\n",
    "    inference_args={'max_length': 200}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c857c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"translate English to Spanish: How are you doing?\",\n",
    "        \"translate English to Spanish: This is the Apache Beam project.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4484c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FormatOutput(beam.DoFn):\n",
    "  \"\"\"\n",
    "  Extract the results from PredictionResult and print the results.\n",
    "  \"\"\"\n",
    "  def process(self, element):\n",
    "    example = element.example\n",
    "    translated_text = element.inference[0]['translation_text']\n",
    "    print(f'Example: {example}')\n",
    "    print(f'Translated text: {translated_text}')\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1997a212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class '__main__.FormatOutput'>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"timestamp\":\"2025-07-15T05:19:10.230435Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Reqwest(reqwest::Error { kind: Request, source: hyper_util::client::legacy::Error(Connect, ConnectError(\\\"dns error\\\", Custom { kind: Uncategorized, error: \\\"failed to lookup address information: nodename nor servname provided, or not known\\\" })) }). Retrying...\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":242}\n",
      "{\"timestamp\":\"2025-07-15T05:19:10.230459Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Retry attempt #0. Sleeping 1.314958139s before the next attempt\"},\"filename\":\"/Users/runner/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs\",\"line_number\":171}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bharathvelamala/Documents/projects/model_inference/ApacheBeamInferenceML/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/Users/bharathvelamala/Documents/projects/model_inference/ApacheBeamInferenceML/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: translate English to Spanish: How are you doing?\n",
      "Translated text: Cómo está acerca?\n",
      "--------------------------------------------------------------------------------\n",
      "Example: translate English to Spanish: This is the Apache Beam project.\n",
      "Translated text: Esto es el proyecto Apache Beam.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as beam_pipeline:\n",
    "  examples = (\n",
    "      beam_pipeline\n",
    "      | \"CreateExamples\" >> beam.Create(text)\n",
    "  )\n",
    "  inferences = (\n",
    "      examples\n",
    "      | \"RunInference\" >> RunInference(model_handler)\n",
    "      | \"Print\" >> beam.ParDo(FormatOutput())\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169fe418",
   "metadata": {},
   "source": [
    "### Masked Language Modeling Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c83c44a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_handler = HuggingFaceModelHandlerKeyedTensor(\n",
    "    model_uri=\"stevhliu/my_awesome_eli5_mlm_model\",\n",
    "    model_class=TFAutoModelForMaskedLM,\n",
    "    framework='tf',\n",
    "    load_model_args={'from_pt': True},\n",
    "    max_batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b911f6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['The capital of France is Paris .',\n",
    "    'It is raining cats and dogs .',\n",
    "    'He looked up and saw the sun and stars .',\n",
    "    'Today is Monday and tomorrow is Tuesday .',\n",
    "    'There are 5 coconuts on this palm tree .']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ed140ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mask_to_last_word(text: str) -> Tuple[str, str]:\n",
    "  \"\"\"Replace the last word of sentence with <mask> and return\n",
    "  the original sentence and the masked sentence.\"\"\"\n",
    "  text_list = text.split()\n",
    "  masked = ' '.join(text_list[:-2] + ['<mask>' + text_list[-1]])\n",
    "  return text, masked\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_eli5_mlm_model\")\n",
    "\n",
    "def tokenize_sentence(\n",
    "    text_and_mask: Tuple[str, str],\n",
    "    tokenizer) -> Tuple[str, Dict[str, tf.Tensor]]:\n",
    "  \"\"\"Convert string examples to tensors.\"\"\"\n",
    "  text, masked_text = text_and_mask\n",
    "  tokenized_sentence = tokenizer.encode_plus(\n",
    "      masked_text, return_tensors=\"tf\")\n",
    "\n",
    "  # Workaround to manually remove batch dim until we have the feature to\n",
    "  # add optional batching flag.\n",
    "  # TODO(https://github.com/apache/beam/issues/21863): Remove when optional\n",
    "  # batching flag added\n",
    "  return text, {\n",
    "      k: tf.squeeze(v)\n",
    "      for k, v in dict(tokenized_sentence).items()\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10dc1824",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcessor(beam.DoFn):\n",
    "  \"\"\"Processes the PredictionResult to get the predicted word.\n",
    "\n",
    "  The logits are the output of the BERT Model. To get the word with the highest\n",
    "  probability of being the masked word, take the argmax.\n",
    "  \"\"\"\n",
    "  def __init__(self, tokenizer):\n",
    "    super().__init__()\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "  def process(self, element: Tuple[str, PredictionResult]) -> Iterable[str]:\n",
    "    text, prediction_result = element\n",
    "    inputs = prediction_result.example\n",
    "    logits = prediction_result.inference['logits']\n",
    "    mask_token_index = tf.where(inputs[\"input_ids\"] == self.tokenizer.mask_token_id)[0]\n",
    "    predicted_token_id = tf.math.argmax(logits[mask_token_index[0]], axis=-1)\n",
    "    decoded_word = self.tokenizer.decode(predicted_token_id)\n",
    "    print(f\"Actual Sentence: {text}\\nPredicted last word: {decoded_word}\")\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "523fdecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class '__main__.PostProcessor'>)\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForMaskedLM: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForMaskedLM from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForMaskedLM from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Sentence: The capital of France is Paris .\n",
      "Predicted last word:  Paris\n",
      "--------------------------------------------------------------------------------\n",
      "Actual Sentence: It is raining cats and dogs .\n",
      "Predicted last word:  dogs\n",
      "--------------------------------------------------------------------------------\n",
      "Actual Sentence: He looked up and saw the sun and stars .\n",
      "Predicted last word:  moon\n",
      "--------------------------------------------------------------------------------\n",
      "Actual Sentence: Today is Monday and tomorrow is Tuesday .\n",
      "Predicted last word:  Tuesday\n",
      "--------------------------------------------------------------------------------\n",
      "Actual Sentence: There are 5 coconuts on this palm tree .\n",
      "Predicted last word:  tree\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as beam_pipeline:\n",
    "  tokenized_examples = (\n",
    "      beam_pipeline\n",
    "      | \"CreateExamples\" >> beam.Create(text)\n",
    "      | 'AddMask' >> beam.Map(add_mask_to_last_word)\n",
    "      | 'TokenizeSentence' >>\n",
    "      beam.Map(lambda x: tokenize_sentence(x, tokenizer)))\n",
    "\n",
    "  result = (\n",
    "      tokenized_examples\n",
    "      | \"RunInference\" >> RunInference(KeyedModelHandler(model_handler))\n",
    "      | \"PostProcess\" >> beam.ParDo(PostProcessor(tokenizer))\n",
    "  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
